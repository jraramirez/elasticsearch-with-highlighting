{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from pandasticsearch import Select\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials\n",
    "\n",
    "credentials = {\n",
    "    \"ip_and_port\": \"127.0.0.1:9200\",\n",
    "    \"username\": \"elastic\",\n",
    "    \"password\": \"Welcometoerni!\"\n",
    "}\n",
    "\n",
    "credentials2 = {\n",
    "    \"ip_and_port\": \"ws-tst-adb.erni2.ch:9200/\",\n",
    "    \"username\": \"elastic\",\n",
    "    \"password\": \"Welcometoerni!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file with the paragraphs\n",
    "f = open(\"../data/rfc3095.txt\")\n",
    "\n",
    "lines = []\n",
    "for line in f:\n",
    "    lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(texts):\n",
    "    return [word for word in simple_preprocess(str(texts)) if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordsLemmatized = []\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process paragraphs\n",
    "paragraphs = []\n",
    "paragraphsOriginal = []\n",
    "paragraphsPreProcessed = []\n",
    "start = 0\n",
    "end = 0\n",
    "for i, line in zip(range(1, len(lines)-1), lines[1:len(lines)-1]):\n",
    "    prevLine = lines[i-1]\n",
    "    currentLine = lines[i]\n",
    "    nextLine = lines[i+1]\n",
    "    if prevLine == \"\\n\":\n",
    "        start = i\n",
    "    if nextLine == \"\\n\":\n",
    "        end = i\n",
    "    if end > start:\n",
    "        paragraph = [x.lower() for x in lines[start:end+1]]\n",
    "        paragraphString = \" \".join(paragraph).replace(\"\\n\", \" \").strip()\n",
    "        paragraphString = re.sub(' +', ' ', paragraphString)\n",
    "        if(len(paragraph) > 0 and not paragraphString in paragraphs):\n",
    "            paragraphs.append(paragraphString)\n",
    "            paragraphsPreProcessed.append(paragraphString)\n",
    "    # if i == 10:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizedParagraphs = []\n",
    "lemmatizedParagraphsStrings = []\n",
    "for paragraph in paragraphs:\n",
    "    lemmatizedParagraph = lemmatize(remove_stopwords(paragraph.lower().split(\" \")))\n",
    "    lemmatizedParagraphs.append(lemmatizedParagraph)\n",
    "    lemmatizedParagraphsStrings.append(\" \".join(lemmatizedParagraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload paragraphs to elasticsearch\n",
    "\n",
    "def addParagraphs(credentials, paragraphsDF, indexName):\n",
    "    actions = [\n",
    "        {\n",
    "            \"_index\": indexName,\n",
    "            \"_source\": {\n",
    "                \"originalParagraph\": row[\"paragraph\"],\n",
    "                \"preProcessedParagraph\": row[\"lemmatizedParagraphString\"]\n",
    "            }\n",
    "        }\n",
    "        for index, row in paragraphsDF.iterrows()\n",
    "    ]\n",
    "    es = Elasticsearch(['http://' + credentials[\"username\"] + ':' + credentials[\"password\"] + '@' + credentials[\"ip_and_port\"]], timeout=600)\n",
    "    helpers.bulk(es, actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload technical standard paragraphs\n",
    "paragraphsDF = pd.DataFrame()\n",
    "paragraphsDF[\"paragraph\"] = paragraphs\n",
    "paragraphsDF[\"lemmatizedParagraphString\"] = lemmatizedParagraphsStrings\n",
    "paragraphsDF[\"lemmatizedParagraph\"] = lemmatizedParagraphs\n",
    "\n",
    "addParagraphs(credentials, paragraphsDF, \"technical-paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search function\n",
    "\n",
    "def searchParagraphs(credentials, inputParagraph, field):\n",
    "    es = Elasticsearch(['http://' + credentials[\"username\"] + ':' + credentials[\"password\"] + '@' + credentials[\"ip_and_port\"]], timeout=600)\n",
    "    doc = {\n",
    "      \"size\" : 500,\n",
    "      \"query\": {\n",
    "        \"multi_match\" : {\n",
    "          \"query\": inputParagraph, \n",
    "          \"fields\": [\n",
    "            field\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    paragraphsDF = pd.DataFrame()\n",
    "    data = es.search(index=\"paragraphs\", body=doc, scroll='1m')\n",
    "    # scrollId = data['_scroll_id']\n",
    "    # scrollSize = len(data['hits']['hits'])\n",
    "    # while scrollSize > 0:\n",
    "    #     if paragraphsDF.empty:\n",
    "    #         paragraphsDF = Select.from_dict(data).to_pandas()\n",
    "    #     else:\n",
    "    #         paragraphsDF = paragraphsDF.append(Select.from_dict(data).to_pandas())\n",
    "    #     data = es.scroll(scroll_id = scrollId, scroll = '1m')\n",
    "    #     scrollId = data['_scroll_id']\n",
    "    #     scrollSize = len(data['hits']['hits'])\n",
    "    # return paragraphsDF\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample search of an input paragraph\n",
    "originalParagraphs = [\n",
    "    \"A header decompression apparatus (709, 908) for decompressing a compressed header of a packet for transmission by referring to reference information being the same as reference information referred to for header compression by a transmitting side, said apparatus (709, 908) comprising\",\n",
    "\n",
    "    \"First, the operation of the header compression apparatus shown in FIG. 1 is specifically described. The packet input part 601 outputs an externally-inputted RTP/UDP/IP packet to the CRC provider 602. The CRC provider 602 computes a CRC for the entire packet, and provides the CRC to the packet.\",\n",
    "\n",
    "    \"A header decompression method for decompressing a compressed header of a packet for transmission by referring to reference information that is the same as reference information referred to for header compression by a transmitting side, said method comprising\",\n",
    "\n",
    "    \"FIG. 6 shows a communication network for wireless terminals over a cellular phone network such as W-CDMA. In recent years, the number of users of such communication network is rapidly growing. The communication network of FIG. 6 includes a wireless transmission section where errors frequently occur. To reduce overhead caused by the header in a wireless section, one header compression scheme is known as ROHC (RObust Header Compression) studied by IETF (Internet Engineering Task Force). The detail of ROHC is described in 'draft-ietf-rohc-rtp-00.txt (29 June 2000)', and in the successive 'draft-ietf-rohc-rtp-01.txt (14 July 2000)'. Further details to said IETF drafts can be found in a posting to the IETF ROHC WG mailing list\",\n",
    "]\n",
    "\n",
    "paragraphs = [p.lower() for p in originalParagraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "paragraphsWithoutStopWords = []\n",
    "for paragraph in paragraphs:\n",
    "    paragraphWords = paragraph.split(\" \")\n",
    "    paragraphWordsWithoutStopWords = remove_stopwords(paragraphWords)\n",
    "    paragraphsWithoutStopWords.append(paragraphWordsWithoutStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "paragraphsLemmatized = []\n",
    "for paragraphWithoutStopWords in paragraphsWithoutStopWords:\n",
    "    paragraphWordsLemmatized = lemmatize(paragraphWithoutStopWords)\n",
    "    paragraphWordsLemmatizedString = \" \".join(paragraphWordsLemmatized)\n",
    "    paragraphsLemmatized.append(paragraphWordsLemmatizedString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload patent document paragraphs\n",
    "paragraphsDF = pd.DataFrame()\n",
    "paragraphsDF[\"paragraph\"] = paragraphs\n",
    "paragraphsDF[\"lemmatizedParagraphString\"] = paragraphsLemmatized\n",
    "\n",
    "addParagraphs(credentials, paragraphsDF, \"patent-paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute search\n",
    "results = searchParagraphs(credentials, paragraphWordsLemmatizedString, \"preProcessedParagraph\")\n",
    "with open('../data/data.json', 'w') as outfile:\n",
    "    json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list words to highlight\n",
    "listOfWordsToHighlight = []\n",
    "for result in results[\"originalParagraph\"]:\n",
    "    wordsToHighlight = []\n",
    "    resultWords = result.split(\" \")\n",
    "    for word in paragraphWordsLemmatized:\n",
    "        if word in resultWords:\n",
    "            wordsToHighlight.append(word)\n",
    "    listOfWordsToHighlight.append(list(set(wordsToHighlight)))\n",
    "results[\"wordsToHighlight\"] = listOfWordsToHighlight\n",
    "\n",
    "results.to_excel(\"../data/searchResults20200714.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}