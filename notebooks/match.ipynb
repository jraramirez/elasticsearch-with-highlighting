{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from pandasticsearch import Select\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials\n",
    "\n",
    "credentials = {\n",
    "    \"ip_and_port\": \"127.0.0.1:9200\",\n",
    "    \"username\": \"elastic\",\n",
    "    \"password\": \"Welcometoerni!\"\n",
    "}\n",
    "\n",
    "credentials2 = {\n",
    "    \"ip_and_port\": \"ws-tst-adb.erni2.ch:9200/\",\n",
    "    \"username\": \"elastic\",\n",
    "    \"password\": \"Welcometoerni!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file with the paragraphs\n",
    "f = open(\"../data/rfc3095.txt\")\n",
    "\n",
    "lines = []\n",
    "for line in f:\n",
    "    lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(texts):\n",
    "    return [word for word in simple_preprocess(str(texts)) if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordsLemmatized = []\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process paragraphs\n",
    "paragraphs = []\n",
    "paragraphsOriginal = []\n",
    "paragraphsPreProcessed = []\n",
    "start = 0\n",
    "end = 0\n",
    "for i, line in zip(range(1, len(lines)-1), lines[1:len(lines)-1]):\n",
    "    prevLine = lines[i-1]\n",
    "    currentLine = lines[i]\n",
    "    nextLine = lines[i+1]\n",
    "    if prevLine == \"\\n\":\n",
    "        start = i\n",
    "    if nextLine == \"\\n\":\n",
    "        end = i\n",
    "    if end > start:\n",
    "        paragraph = [x for x in lines[start:end+1]]\n",
    "        paragraphString = \" \".join(paragraph).replace(\"\\n\", \" \").strip()\n",
    "        paragraphString = re.sub(' +', ' ', paragraphString)\n",
    "        if(len(paragraph) > 0 and not paragraphString in paragraphs):\n",
    "            paragraphs.append(paragraphString)\n",
    "            paragraphsPreProcessed.append(paragraphString)\n",
    "    # if i == 10:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizedParagraphs = []\n",
    "lemmatizedParagraphsStrings = []\n",
    "for paragraph in paragraphs:\n",
    "    lemmatizedParagraph = lemmatize(remove_stopwords(paragraph.lower().split(\" \")))\n",
    "    lemmatizedParagraphs.append(lemmatizedParagraph)\n",
    "    lemmatizedParagraphsStrings.append(\" \".join(lemmatizedParagraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload paragraphs to elasticsearch\n",
    "\n",
    "def addParagraphs(credentials, paragraphsDF, indexName):\n",
    "    actions = [\n",
    "        {\n",
    "            \"_index\": indexName,\n",
    "            \"_source\": {\n",
    "                \"originalParagraph\": row[\"paragraph\"],\n",
    "                \"preProcessedParagraph\": row[\"lemmatizedParagraphString\"]\n",
    "            }\n",
    "        }\n",
    "        for index, row in paragraphsDF.iterrows()\n",
    "    ]\n",
    "    es = Elasticsearch(['http://' + credentials[\"username\"] + ':' + credentials[\"password\"] + '@' + credentials[\"ip_and_port\"]], timeout=600)\n",
    "    helpers.bulk(es, actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload technical standard paragraphs\n",
    "paragraphsDF = pd.DataFrame()\n",
    "paragraphsDF[\"paragraph\"] = paragraphs\n",
    "paragraphsDF[\"lemmatizedParagraphString\"] = lemmatizedParagraphsStrings\n",
    "paragraphsDF[\"lemmatizedParagraph\"] = lemmatizedParagraphs\n",
    "\n",
    "addParagraphs(credentials2, paragraphsDF, \"technical-paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search function\n",
    "\n",
    "def searchParagraphs(credentials, inputParagraph, field):\n",
    "    es = Elasticsearch(['http://' + credentials[\"username\"] + ':' + credentials[\"password\"] + '@' + credentials[\"ip_and_port\"]], timeout=600)\n",
    "    doc = {\n",
    "      \"size\" : 500,\n",
    "      \"query\": {\n",
    "        \"multi_match\" : {\n",
    "          \"query\": inputParagraph, \n",
    "          \"fields\": [\n",
    "            field\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    paragraphsDF = pd.DataFrame()\n",
    "    data = es.search(index=\"paragraphs\", body=doc, scroll='1m')\n",
    "    # scrollId = data['_scroll_id']\n",
    "    # scrollSize = len(data['hits']['hits'])\n",
    "    # while scrollSize > 0:\n",
    "    #     if paragraphsDF.empty:\n",
    "    #         paragraphsDF = Select.from_dict(data).to_pandas()\n",
    "    #     else:\n",
    "    #         paragraphsDF = paragraphsDF.append(Select.from_dict(data).to_pandas())\n",
    "    #     data = es.scroll(scroll_id = scrollId, scroll = '1m')\n",
    "    #     scrollId = data['_scroll_id']\n",
    "    #     scrollSize = len(data['hits']['hits'])\n",
    "    # return paragraphsDF\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample search of an input paragraph\n",
    "originalParagraphs = [\n",
    "    \"A header decompression apparatus (709, 908) for decompressing a compressed header of a packet for transmission by referring to reference information being the same as reference information referred to for header compression by a transmitting side, said apparatus (709, 908) comprising\",\n",
    "\n",
    "    \"The header decompression apparatus according to claim 1, wherein when X â‰¥ Y, said update request unit (708) determines that the reference information stored in said reference information manager (707) should be updated.\",\n",
    "\n",
    "    \"A header decompression method for decompressing a compressed header of a packet for transmission by referring to reference information that is the same as reference information referred to for header compression by a transmitting side, said method comprising\"\n",
    "]\n",
    "\n",
    "paragraphs = [p.lower() for p in originalParagraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[['header',\n  'decompression',\n  'apparatus',\n  'decompressing',\n  'compressed',\n  'header',\n  'packet',\n  'transmission',\n  'referring',\n  'reference',\n  'information',\n  'reference',\n  'information',\n  'referred',\n  'header',\n  'compression',\n  'transmitting',\n  'side',\n  'said',\n  'apparatus',\n  'comprising'],\n ['header',\n  'decompression',\n  'apparatus',\n  'according',\n  'claim',\n  'wherein',\n  'said',\n  'update',\n  'request',\n  'unit',\n  'determines',\n  'reference',\n  'information',\n  'stored',\n  'said',\n  'reference',\n  'information',\n  'manager',\n  'updated'],\n ['header',\n  'decompression',\n  'method',\n  'decompressing',\n  'compressed',\n  'header',\n  'packet',\n  'transmission',\n  'referring',\n  'reference',\n  'information',\n  'reference',\n  'information',\n  'referred',\n  'header',\n  'compression',\n  'transmitting',\n  'side',\n  'said',\n  'method',\n  'comprising']]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "paragraphsWithoutStopWords = []\n",
    "for paragraph in paragraphs:\n",
    "    paragraphWords = paragraph.split(\" \")\n",
    "    paragraphWordsWithoutStopWords = remove_stopwords(paragraphWords)\n",
    "    paragraphsWithoutStopWords.append(paragraphWordsWithoutStopWords)\n",
    "paragraphsWithoutStopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['header decompression apparatus decompress compress header packet transmission refer reference information reference information refer header compression transmit side say apparatus comprise',\n 'header decompression apparatus accord claim wherein say update request unit determines reference information store say reference information manager update',\n 'header decompression method decompress compress header packet transmission refer reference information reference information refer header compression transmit side say method comprise']"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize\n",
    "paragraphsLemmatized = []\n",
    "for paragraphWithoutStopWords in paragraphsWithoutStopWords:\n",
    "    paragraphWordsLemmatized = lemmatize(paragraphWithoutStopWords)\n",
    "    paragraphWordsLemmatizedString = \" \".join(paragraphWordsLemmatized)\n",
    "    paragraphsLemmatized.append(paragraphWordsLemmatizedString)\n",
    "paragraphsLemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload patent document paragraphs\n",
    "paragraphsDF = pd.DataFrame()\n",
    "paragraphsDF[\"paragraph\"] = originalParagraphs\n",
    "paragraphsDF[\"lemmatizedParagraphString\"] = paragraphsLemmatized\n",
    "\n",
    "addParagraphs(credentials2, paragraphsDF, \"patent-paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute search\n",
    "results = searchParagraphs(credentials, paragraphWordsLemmatizedString, \"preProcessedParagraph\")\n",
    "with open('../data/data.json', 'w') as outfile:\n",
    "    json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list words to highlight\n",
    "listOfWordsToHighlight = []\n",
    "for result in results[\"originalParagraph\"]:\n",
    "    wordsToHighlight = []\n",
    "    resultWords = result.split(\" \")\n",
    "    for word in paragraphWordsLemmatized:\n",
    "        if word in resultWords:\n",
    "            wordsToHighlight.append(word)\n",
    "    listOfWordsToHighlight.append(list(set(wordsToHighlight)))\n",
    "results[\"wordsToHighlight\"] = listOfWordsToHighlight\n",
    "\n",
    "results.to_excel(\"../data/searchResults20200714.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}